yarn：RM NM
hadoop000 192.168.137.2
NN RM
DN NM

hadoop001 192.168.137.3
DN NM
hadoop002 192.168.137.4
DN NM

(每台)
/etc/hostname：修改hostname(hadoop000/hadoop001/hadoop002)
/etc/hosts：ip和hostname的映射关系
192.168.137.2 hadoop000
192.168.137.3 hadoop001
192.168.137.4 hadoop002
192.168.137.2 localhost

(每台)
前置安装 ssh
ssh免密码登录：ssh-keygen -t rsa

在hadoop000机器上进行操作
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop000
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop001
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop002

jdk安装
1) 先在hadoop000机器上部署了jdk
2) 将jdk bin配置到系统环境变量
3) 将jdkcopy到其他节点上去(从hadoop000机器出发)
scp -r jdk1.8.0_91 hadoop@hadoop001:~/app/
scp -r jdk1.8.0_91 hadoop@hadoop002:~/app/

scp  ~/.bash_profile hadoop@hadoop001:~/
scp  ~/.bash_profile hadoop@hadoop002:~/

每台机器 source ~/.bash_profile

hadoop部署
	1) hadoop-env.sh   JAVA_HOME
	2) core-site.xml
		<property>
			<name>fs.default.name</name>
			<value>hdfs://hadoop000:8020</value>
		</property>

	3) hdfs-site.xml
		hadoop namenode\datanode数据临时目录
		副本系数
			<property>
			  <name>dfs.namenode.name.dir</name>
			  <value>/home/hadoop/app/tmp/dfs/name</value>
			</property>

			<property>
			  <name>dfs.datanode.data.dir</name>
			  <value>/home/hadoop/app/tmp/dfs/data</value>
			</property>
	4) yarn-site.xml
		resourcemanager在哪台机器
			<property>
			  <name>yarn.nodemanager.aux-services</name>
			  <value>mapreduce_shuffle</value>
			 </property>

			<property>
				<name>yarn.resourcemanager.hostname</name>
				<value>hadoop000</value>
			</property>
	5) mapred-site.xml
			<property>
				<name>mapreduce.framework.name</name>
				<value>yarn</value>
			</property>

	6) slaves
		hadoop000 的slave上配置hadoop000、hadoop001、hadoop002，其他机器不用配置
	7) 将hadoop配置到其他机器上
		scp -r hadoop-2.6.0-cdh5.15.1 hadoop@hadoop001:~/app/
		scp -r hadoop-2.6.0-cdh5.15.1 hadoop@hadoop002:~/app/

		scp ~/.bash_profile hadoop@hadoop001:~/
		scp ~/.bash_profile hadoop@hadoop002:~/

	8) hadoop namenode -format

	9) 启动hdfs

	10) 启动yarn
